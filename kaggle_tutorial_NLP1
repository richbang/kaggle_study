{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8780802,"sourceType":"datasetVersion","datasetId":5278067}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keras-core --upgrade\n!pip install -q keras-nlp -upgrade\n\nimport os\nos.environ['KERAS_BACKEND'] = 'tensorflow'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-25T08:36:48.990567Z","iopub.execute_input":"2024-06-25T08:36:48.990980Z","iopub.status.idle":"2024-06-25T08:37:09.817038Z","shell.execute_reply.started":"2024-06-25T08:36:48.990941Z","shell.execute_reply":"2024-06-25T08:37:09.815654Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting keras-core\n  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.26.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-core) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras-core) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core) (3.10.0)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-core) (0.1.8)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-core) (0.1.2)\nDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras-core\nSuccessfully installed keras-core-0.1.7\n\nUsage:   \n  pip install [options] <requirement specifier> [package-index-options] ...\n  pip install [options] -r <requirements file> [package-index-options] ...\n  pip install [options] [-e] <vcs project url> ...\n  pip install [options] [-e] <local project path> ...\n  pip install [options] <archive url/path> ...\n\nno such option: -u\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras_core as keras\nimport keras_nlp\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Keras NLP version:\", keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T08:38:55.419220Z","iopub.execute_input":"2024-06-25T08:38:55.419803Z","iopub.status.idle":"2024-06-25T08:39:15.209637Z","shell.execute_reply.started":"2024-06-25T08:38:55.419763Z","shell.execute_reply":"2024-06-25T08:39:15.208195Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-25 08:38:58.369158: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-25 08:38:58.369310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-25 08:38:58.552390: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using TensorFlow backend\nTensorFlow version: 2.15.0\nKeras NLP version: 0.12.1\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/disaster-tweets/kaggle/Natural Language Processing with Disaster Tweets/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/disaster-tweets/kaggle/Natural Language Processing with Disaster Tweets/test.csv\")\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))","metadata":{"execution":{"iopub.status.busy":"2024-06-25T08:48:39.292672Z","iopub.execute_input":"2024-06-25T08:48:39.293258Z","iopub.status.idle":"2024-06-25T08:48:39.387267Z","shell.execute_reply.started":"2024-06-25T08:48:39.293206Z","shell.execute_reply":"2024-06-25T08:48:39.385833Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Training Set Shape = (7613, 5)\nTraining Set Memory Usage = 0.29 MB\nTest Set Shape = (3263, 4)\nTest Set Memory Usage = 0.10 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train.head()\n\n# 실제 재해를 설명하는 경우 1\n# 그렇지 않은 경우 0","metadata":{"execution":{"iopub.status.busy":"2024-06-25T08:48:42.579987Z","iopub.execute_input":"2024-06-25T08:48:42.581470Z","iopub.status.idle":"2024-06-25T08:48:42.603220Z","shell.execute_reply.started":"2024-06-25T08:48:42.581392Z","shell.execute_reply":"2024-06-25T08:48:42.601646Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_test.head()\n\n# 실제 재해를 설명하는 경우 1\n# 그렇지 않은 경우 0","metadata":{"execution":{"iopub.status.busy":"2024-06-25T08:56:07.752996Z","iopub.execute_input":"2024-06-25T08:56:07.754328Z","iopub.status.idle":"2024-06-25T08:56:07.768095Z","shell.execute_reply.started":"2024-06-25T08:56:07.754286Z","shell.execute_reply":"2024-06-25T08:56:07.766720Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_train[\"length\"] = df_train[\"text\"].apply(lamda x: len(x))\ndf_test[\"length\"] = df_test[\"text\"].apply(lamda x: len(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**-------------------------------------------------------------------------------------------------------------------------------------------------**","metadata":{}},{"cell_type":"code","source":"# 데이터 로드\ntrain_data_path = \"/kaggle/input/disaster-tweets/kaggle/Natural Language Processing with Disaster Tweets/train.csv\"\ntest_data_path = \"/kaggle/input/disaster-tweets/kaggle/Natural Language Processing with Disaster Tweets/test.csv\"\nsample_submission_path = \"/kaggle/input/disaster-tweets/kaggle/Natural Language Processing with Disaster Tweets/sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:27:04.251760Z","iopub.execute_input":"2024-06-26T06:27:04.252224Z","iopub.status.idle":"2024-06-26T06:27:04.477413Z","shell.execute_reply.started":"2024-06-26T06:27:04.252177Z","shell.execute_reply":"2024-06-26T06:27:04.475972Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import get_tokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import f1_score\n\n# 데이터셋 정의\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, vocab):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        tokens = self.tokenizer(text)\n        token_ids = [self.vocab[token] for token in tokens]\n        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n\n# 패딩 함수 정의\ndef pad_collate(batch):\n    texts, labels = zip(*batch)\n    lengths = torch.tensor([len(x) for x in texts])\n    padded_texts = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>'])\n    labels = torch.tensor(labels)\n    return padded_texts, lengths, labels\n\n# 1. 데이터 로드 및 확인\ntrain_data_path = \"/kaggle/input/disaster-tweets/kaggle/Natural Language Processing with Disaster Tweets/train.csv\"\ntest_data_path = \"/kaggle/input/disaster-tweets/kaggle/Natural Language Processing with Disaster Tweets/test.csv\"\nsample_submission_path = \"/kaggle/input/disaster-tweets/kaggle/Natural Language Processing with Disaster Tweets/sample_submission.csv\"\n\ntrain_data = pd.read_csv(train_data_path)\ntest_data = pd.read_csv(test_data_path)\nsample_submission = pd.read_csv(sample_submission_path)\n\n# 결측값 처리 및 텍스트 결합\ntrain_data['keyword'] = train_data['keyword'].fillna('')\ntrain_data['text'] = train_data['text'].fillna('')\ntrain_data['combined_text'] = train_data['keyword'] + ' ' + train_data['text']\ntrain_data.to_csv('processed_train_data.csv', index=False)\n\ntest_data['keyword'] = test_data['keyword'].fillna('')\ntest_data['text'] = test_data['text'].fillna('')\ntest_data['combined_text'] = test_data['keyword'] + ' ' + test_data['text']\ntest_data.to_csv('processed_test_data.csv', index=False)\n\n# 2. 데이터 전처리 및 준비\ntokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n\n# 어휘 사전 빌드\ndef yield_tokens(data_iter):\n    for text in data_iter:\n        yield tokenizer(text)\n\nvocab = build_vocab_from_iterator(yield_tokens(train_data['combined_text']), specials=[\"<unk>\", \"<pad>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\n\n# 훈련셋과 검증셋 분리\ntrain_texts, valid_texts, train_labels, valid_labels = train_test_split(\n    train_data['combined_text'].tolist(),\n    train_data['target'].tolist(),\n    test_size=0.2,\n    random_state=42\n)\n\ntrain_dataset = TextDataset(train_texts, train_labels, tokenizer, vocab)\nvalid_dataset = TextDataset(valid_texts, valid_labels, tokenizer, vocab)\ntest_dataset = TextDataset(test_data['combined_text'].tolist(), [0]*len(test_data), tokenizer, vocab)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\n\n# 3. 모델 설계\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n        super(LSTMClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text, text_lengths):\n        embedded = self.dropout(self.embedding(text))\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), batch_first=True, enforce_sorted=False)\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        if self.lstm.bidirectional:\n            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n        else:\n            hidden = self.dropout(hidden[-1,:,:])\n        return self.fc(hidden)\n\n# 모델 하이퍼파라미터 정의\nINPUT_DIM = len(vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 3\nBIDIRECTIONAL = True\nDROPOUT = 0.5\n\nmodel = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n\n# 옵티마이저와 손실 함수 정의\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.BCEWithLogitsLoss()\n\n# 모델을 GPU로 이동\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\ncriterion = criterion.to(device)\n\n# 4. 훈련 및 평가 함수 정의\ndef binary_accuracy(preds, y):\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float()\n    acc = correct.sum() / len(correct)\n    return acc\n\ndef train(model, iterator, optimizer, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    model.train()\n    \n    for batch in iterator:\n        optimizer.zero_grad()\n        text, text_lengths, labels = batch\n        text, text_lengths, labels = text.to(device), text_lengths.to(device), labels.to(device)\n        predictions = model(text, text_lengths).squeeze(1)\n        loss = criterion(predictions, labels.float())\n        acc = binary_accuracy(predictions, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for batch in iterator:\n            text, text_lengths, labels = batch\n            text, text_lengths, labels = text.to(device), text_lengths.to(device), labels.to(device)\n            predictions = model(text, text_lengths).squeeze(1)\n            loss = criterion(predictions, labels.float())\n            acc = binary_accuracy(predictions, labels)\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n            \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef evaluate_f1(model, iterator):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in iterator:\n            text, text_lengths, labels = batch\n            text, text_lengths, labels = text.to(device), text_lengths.to(device), labels.to(device)\n            predictions = model(text, text_lengths).squeeze(1)\n            rounded_preds = torch.round(torch.sigmoid(predictions))\n            all_preds.extend(rounded_preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n    return f1_score(all_labels, all_preds)\n\n# 모델 훈련\nN_EPOCHS = 30\n\nfor epoch in range(N_EPOCHS):\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n    valid_f1 = evaluate_f1(model, valid_loader)\n    \n    print(f'Epoch: {epoch+1:02}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val. F1: {valid_f1:.2f}')\n\n# 테스트 데이터 예측 및 제출 파일 생성\ndef predict(model, iterator):\n    model.eval()\n    all_preds = []\n    \n    with torch.no_grad():\n        for batch in iterator:\n            text, text_lengths, _ = batch\n            text, text_lengths = text.to(device), text_lengths.to(device)\n            predictions = model(text, text_lengths).squeeze(1)\n            rounded_preds = torch.round(torch.sigmoid(predictions))\n            all_preds.extend(rounded_preds.cpu().numpy())\n            \n    return all_preds\n\ntest_preds = predict(model, test_loader)\nsample_submission['target'] = test_preds\nsample_submission.to_csv('submission_30epoch.csv', index=False)\n\nprint('Prediction results saved to submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:54:18.211962Z","iopub.execute_input":"2024-06-26T08:54:18.212519Z","iopub.status.idle":"2024-06-26T09:28:03.241896Z","shell.execute_reply.started":"2024-06-26T08:54:18.212483Z","shell.execute_reply":"2024-06-26T09:28:03.240728Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Epoch: 01\n\tTrain Loss: 0.646 | Train Acc: 62.99%\n\t Val. Loss: 0.583 |  Val. Acc: 71.18% | Val. F1: 0.64\nEpoch: 02\n\tTrain Loss: 0.557 | Train Acc: 72.30%\n\t Val. Loss: 0.517 |  Val. Acc: 74.19% | Val. F1: 0.68\nEpoch: 03\n\tTrain Loss: 0.506 | Train Acc: 76.35%\n\t Val. Loss: 0.497 |  Val. Acc: 76.62% | Val. F1: 0.72\nEpoch: 04\n\tTrain Loss: 0.466 | Train Acc: 78.74%\n\t Val. Loss: 0.491 |  Val. Acc: 76.88% | Val. F1: 0.71\nEpoch: 05\n\tTrain Loss: 0.424 | Train Acc: 80.77%\n\t Val. Loss: 0.520 |  Val. Acc: 76.62% | Val. F1: 0.73\nEpoch: 06\n\tTrain Loss: 0.392 | Train Acc: 82.88%\n\t Val. Loss: 0.506 |  Val. Acc: 77.64% | Val. F1: 0.72\nEpoch: 07\n\tTrain Loss: 0.359 | Train Acc: 84.54%\n\t Val. Loss: 0.532 |  Val. Acc: 76.47% | Val. F1: 0.73\nEpoch: 08\n\tTrain Loss: 0.324 | Train Acc: 86.30%\n\t Val. Loss: 0.552 |  Val. Acc: 76.51% | Val. F1: 0.72\nEpoch: 09\n\tTrain Loss: 0.296 | Train Acc: 87.84%\n\t Val. Loss: 0.580 |  Val. Acc: 75.69% | Val. F1: 0.71\nEpoch: 10\n\tTrain Loss: 0.282 | Train Acc: 88.15%\n\t Val. Loss: 0.616 |  Val. Acc: 75.82% | Val. F1: 0.72\nEpoch: 11\n\tTrain Loss: 0.244 | Train Acc: 90.09%\n\t Val. Loss: 0.667 |  Val. Acc: 78.73% | Val. F1: 0.73\nEpoch: 12\n\tTrain Loss: 0.227 | Train Acc: 91.35%\n\t Val. Loss: 0.629 |  Val. Acc: 77.28% | Val. F1: 0.73\nEpoch: 13\n\tTrain Loss: 0.202 | Train Acc: 91.78%\n\t Val. Loss: 0.788 |  Val. Acc: 73.28% | Val. F1: 0.71\nEpoch: 14\n\tTrain Loss: 0.181 | Train Acc: 92.83%\n\t Val. Loss: 0.704 |  Val. Acc: 75.34% | Val. F1: 0.72\nEpoch: 15\n\tTrain Loss: 0.177 | Train Acc: 93.06%\n\t Val. Loss: 0.807 |  Val. Acc: 76.27% | Val. F1: 0.72\nEpoch: 16\n\tTrain Loss: 0.152 | Train Acc: 94.06%\n\t Val. Loss: 0.897 |  Val. Acc: 75.62% | Val. F1: 0.72\nEpoch: 17\n\tTrain Loss: 0.134 | Train Acc: 95.14%\n\t Val. Loss: 0.883 |  Val. Acc: 74.36% | Val. F1: 0.71\nEpoch: 18\n\tTrain Loss: 0.132 | Train Acc: 94.73%\n\t Val. Loss: 0.924 |  Val. Acc: 74.21% | Val. F1: 0.71\nEpoch: 19\n\tTrain Loss: 0.122 | Train Acc: 95.35%\n\t Val. Loss: 0.916 |  Val. Acc: 74.54% | Val. F1: 0.72\nEpoch: 20\n\tTrain Loss: 0.119 | Train Acc: 95.64%\n\t Val. Loss: 0.980 |  Val. Acc: 74.02% | Val. F1: 0.71\nEpoch: 21\n\tTrain Loss: 0.117 | Train Acc: 95.78%\n\t Val. Loss: 1.016 |  Val. Acc: 74.10% | Val. F1: 0.71\nEpoch: 22\n\tTrain Loss: 0.106 | Train Acc: 96.01%\n\t Val. Loss: 0.988 |  Val. Acc: 76.17% | Val. F1: 0.73\nEpoch: 23\n\tTrain Loss: 0.103 | Train Acc: 96.19%\n\t Val. Loss: 1.000 |  Val. Acc: 74.78% | Val. F1: 0.72\nEpoch: 24\n\tTrain Loss: 0.091 | Train Acc: 96.32%\n\t Val. Loss: 1.075 |  Val. Acc: 76.43% | Val. F1: 0.71\nEpoch: 25\n\tTrain Loss: 0.095 | Train Acc: 96.45%\n\t Val. Loss: 1.069 |  Val. Acc: 74.88% | Val. F1: 0.71\nEpoch: 26\n\tTrain Loss: 0.088 | Train Acc: 96.77%\n\t Val. Loss: 1.111 |  Val. Acc: 74.34% | Val. F1: 0.71\nEpoch: 27\n\tTrain Loss: 0.074 | Train Acc: 97.04%\n\t Val. Loss: 1.156 |  Val. Acc: 74.41% | Val. F1: 0.71\nEpoch: 28\n\tTrain Loss: 0.073 | Train Acc: 97.25%\n\t Val. Loss: 1.135 |  Val. Acc: 75.38% | Val. F1: 0.71\nEpoch: 29\n\tTrain Loss: 0.073 | Train Acc: 97.10%\n\t Val. Loss: 1.144 |  Val. Acc: 74.60% | Val. F1: 0.72\nEpoch: 30\n\tTrain Loss: 0.078 | Train Acc: 97.15%\n\t Val. Loss: 1.132 |  Val. Acc: 75.36% | Val. F1: 0.72\nPrediction results saved to submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import get_tokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import f1_score\n\n# 데이터셋 정의\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, vocab):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        tokens = self.tokenizer(text)\n        token_ids = [self.vocab[token] for token in tokens]\n        return torch.tensor(token_ids, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n\n# 패딩 함수 정의\ndef pad_collate(batch):\n    texts, labels = zip(*batch)\n    lengths = torch.tensor([len(x) for x in texts])\n    padded_texts = torch.nn.utils.rnn.pad_sequence(texts, batch_first=True, padding_value=vocab['<pad>'])\n    labels = torch.tensor(labels)\n    return padded_texts, lengths, labels\n\n# 1. 데이터 로드 및 확인\ntrain_data_path = \"/kaggle/input/disaster-tweets/kaggle/Natural Language Processing with Disaster Tweets/train.csv\"\ntest_data_path = \"/kaggle/input/disaster-tweets/kaggle/Natural Language Processing with Disaster Tweets/test.csv\"\nsample_submission_path = \"/kaggle/input/disaster-tweets/kaggle/Natural Language Processing with Disaster Tweets/sample_submission.csv\"\n\ntrain_data = pd.read_csv(train_data_path)\ntest_data = pd.read_csv(test_data_path)\nsample_submission = pd.read_csv(sample_submission_path)\n\n# 결측값 처리 및 텍스트 결합\ntrain_data['keyword'] = train_data['keyword'].fillna('')\ntrain_data['text'] = train_data['text'].fillna('')\ntrain_data['combined_text'] = train_data['keyword'] + ' ' + train_data['text']\ntrain_data.to_csv('processed_train_data.csv', index=False)\n\ntest_data['keyword'] = test_data['keyword'].fillna('')\ntest_data['text'] = test_data['text'].fillna('')\ntest_data['combined_text'] = test_data['keyword'] + ' ' + test_data['text']\ntest_data.to_csv('processed_test_data.csv', index=False)\n\n# 2. 데이터 전처리 및 준비\ntokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n\n# 어휘 사전 빌드\ndef yield_tokens(data_iter):\n    for text in data_iter:\n        yield tokenizer(text)\n\nvocab = build_vocab_from_iterator(yield_tokens(train_data['combined_text']), specials=[\"<unk>\", \"<pad>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\n\n# 훈련셋과 검증셋 분리\ntrain_texts, valid_texts, train_labels, valid_labels = train_test_split(\n    train_data['combined_text'].tolist(),\n    train_data['target'].tolist(),\n    test_size=0.2,\n    random_state=42\n)\n\ntrain_dataset = TextDataset(train_texts, train_labels, tokenizer, vocab)\nvalid_dataset = TextDataset(valid_texts, valid_labels, tokenizer, vocab)\ntest_dataset = TextDataset(test_data['combined_text'].tolist(), [0]*len(test_data), tokenizer, vocab)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)\nvalid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)\n\n# 3. 모델 설계\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n        super(LSTMClassifier, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text, text_lengths):\n        embedded = self.dropout(self.embedding(text))\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), batch_first=True, enforce_sorted=False)\n        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n        if self.lstm.bidirectional:\n            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n        else:\n            hidden = self.dropout(hidden[-1,:,:])\n        return self.fc(hidden)\n\n# 모델 하이퍼파라미터 정의\nINPUT_DIM = len(vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.5\n\nmodel = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n\n# 옵티마이저와 손실 함수 정의\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.BCEWithLogitsLoss()\n\n# 모델을 GPU로 이동\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\ncriterion = criterion.to(device)\n\n# 4. 훈련 및 평가 함수 정의\ndef binary_accuracy(preds, y):\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float()\n    acc = correct.sum() / len(correct)\n    return acc\n\ndef train(model, iterator, optimizer, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    model.train()\n    \n    for batch in iterator:\n        optimizer.zero_grad()\n        text, text_lengths, labels = batch\n        text, text_lengths, labels = text.to(device), text_lengths.to(device), labels.to(device)\n        predictions = model(text, text_lengths).squeeze(1)\n        loss = criterion(predictions, labels.float())\n        acc = binary_accuracy(predictions, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for batch in iterator:\n            text, text_lengths, labels = batch\n            text, text_lengths, labels = text.to(device), text_lengths.to(device), labels.to(device)\n            predictions = model(text, text_lengths).squeeze(1)\n            loss = criterion(predictions, labels.float())\n            acc = binary_accuracy(predictions, labels)\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n            \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\ndef evaluate_f1(model, iterator):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in iterator:\n            text, text_lengths, labels = batch\n            text, text_lengths, labels = text.to(device), text_lengths.to(device), labels.to(device)\n            predictions = model(text, text_lengths).squeeze(1)\n            rounded_preds = torch.round(torch.sigmoid(predictions))\n            all_preds.extend(rounded_preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n    return f1_score(all_labels, all_preds)\n\n# 모델 훈련\nN_EPOCHS = 30\n\nfor epoch in range(N_EPOCHS):\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n    valid_f1 = evaluate_f1(model, valid_loader)\n    \n    print(f'Epoch: {epoch+1:02}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% | Val. F1: {valid_f1:.2f}')\n\n# 테스트 데이터 예측 및 제출 파일 생성\ndef predict(model, iterator):\n    model.eval()\n    all_preds = []\n    \n    with torch.no_grad():\n        for batch in iterator:\n            text, text_lengths, _ = batch\n            text, text_lengths = text.to(device), text_lengths.to(device)\n            predictions = model(text, text_lengths).squeeze(1)\n            rounded_preds = torch.round(torch.sigmoid(predictions))\n            all_preds.extend(rounded_preds.cpu().numpy())\n            \n    return all_preds\n\ntest_preds = predict(model, test_loader)\nsample_submission['target'] = test_preds\nsample_submission.to_csv('submission_new.csv', index=False)\n\nprint('Prediction results saved to submission.csv')","metadata":{},"execution_count":null,"outputs":[]}]}